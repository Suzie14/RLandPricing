{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, alpha=0.05, beta=10**(-5), delta=0.99):\n",
    "        self.A = ['c', 'd']\n",
    "\n",
    "        self.epsilon = 1\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.delta = delta\n",
    "        \n",
    "\n",
    "        self.s_ind = None\n",
    "        self.s_t = None\n",
    "        self.a_ind = None\n",
    "        self.a = None\n",
    "        self.s_ind1 = None\n",
    "        self.s_t1 = None\n",
    "\n",
    "        self.Q = np.random.uniform(size=(2, 4), low=-0.5, high=0.5)\n",
    "\n",
    "\n",
    "    def get_next_action(self):\n",
    "        if np.random.random() < 1-self.epsilon:\n",
    "            self.a_ind = self.Q[:, self.s_ind].argmax()\n",
    "        else:\n",
    "            self.a_ind = np.random.randint(2)\n",
    "        \n",
    "        self.a = self.A[self.a_ind]\n",
    "\n",
    "\n",
    "    def updateQ(self, reward, t):  # upateQ\n",
    "        \n",
    "        self.Q[self.a_ind, self.s_ind] = (\n",
    "                1-self.alpha)*self.Q[self.a_ind, self.s_ind] + self.alpha*(reward + self.delta*self.Q[:, self.s_ind1].max())\n",
    "\n",
    "        self.s_ind = self.s_ind1\n",
    "        self.epsilon = np.exp(-self.beta*t)\n",
    "\n",
    "class Env:\n",
    "    def __init__(self, r_dd=-2, r_cd=-3, r_cc=-1, r_dc=0):\n",
    "        self.r_dd = r_dd\n",
    "        self.r_cd = r_cd\n",
    "        self.r_cc = r_cc\n",
    "        self.r_dc = r_dc\n",
    "\n",
    "    def __call__(self, action1, action2):\n",
    "        if action1+action2 == 'dd':\n",
    "            return self.r_dd, 0\n",
    "        elif action1+action2 == 'cd':\n",
    "            return self.r_cd, 1\n",
    "        elif action1+action2 == 'cc':\n",
    "            return self.r_cc, 2\n",
    "        elif action1+action2 == 'dc':\n",
    "            return self.r_dc, 3\n",
    "        else: \n",
    "            raise ValueError(\"Actions should be 'd' or 'c'\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cc\n",
      "[[-100.184723   -109.60674433 -100.         -108.80998279]\n",
      " [-105.22799562 -101.3088878  -100.57746434 -101.33321468]] [[-100.14106092 -109.65096523 -100.0001923  -110.86839843]\n",
      " [-104.82241599 -101.36856802 -100.582943   -101.33608918]]\n"
     ]
    }
   ],
   "source": [
    "agent1 = Agent()\n",
    "agent2 = Agent()\n",
    "environment = Env()\n",
    "\n",
    "agent1.get_next_action()\n",
    "agent2.get_next_action()\n",
    "\n",
    "reward1, agent1.s_ind = environment(agent1.a, agent2.a)\n",
    "reward2, agent2.s_ind = environment(agent1.a, agent2.a)\n",
    "\n",
    "\n",
    "\n",
    "for t in range(1000000):\n",
    "    agent1.updateQ(reward1, t)\n",
    "    agent2.updateQ(reward2, t)\n",
    "\n",
    "    agent1.get_next_action()\n",
    "    agent2.get_next_action()\n",
    "\n",
    "    reward1, agent1.s_ind1 = environment(agent1.a, agent2.a)\n",
    "    reward2, agent2.s_ind1 = environment(agent2.a, agent1.a)\n",
    "\n",
    "print(agent1.a+agent2.a)\n",
    "print(agent1.Q, agent2.Q)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
