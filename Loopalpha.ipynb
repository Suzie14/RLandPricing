{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import QLearningAlgo as Q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variation du $\\alpha$:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuition : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "aggregated_agent1 = []\n",
    "aggregated_agent2 = []\n",
    "for alpha in [0.1,0.2,0.3,0.4,0.5]:\n",
    "    total_reward1 = []\n",
    "    total_reward2 = []\n",
    "    for loop in range(3):\n",
    "        print(loop)\n",
    "        agent1 = Q.Agent(alpha=alpha)\n",
    "        agent2 = Q.Agent(alpha=alpha)\n",
    "        env = Q.Env()\n",
    "    \n",
    "        temps = []\n",
    "        reward1 = []\n",
    "        reward2 = []\n",
    "        epsilon = []\n",
    "        prices1 = []\n",
    "        prices2 = []\n",
    "\n",
    "        #1) On initialise p0 (on va le faire direct dans agent)\n",
    "        agent1.p = np.random.choice(agent1.A)\n",
    "        agent2.p = np.random.choice(agent2.A)\n",
    "        #Initialisation de l'etat\n",
    "        #s_t\n",
    "        s_t = env([agent1.p,agent2.p])[1]\n",
    "        agent1.s_t = s_t #par rapport à p #ENV car l'aent interract seulement avec l'env et pas l'autre agent\n",
    "        agent2.s_t = s_t\n",
    "\n",
    "        s_ind =  agent1.find_index(agent1.S, agent1.s_t) #même indice pour les 2 agents\n",
    "        agent1.s_ind = s_ind\n",
    "        agent2.s_ind = s_ind\n",
    "\n",
    "\n",
    "        #Phase itérative\n",
    "        for t in range(10**(6)):\n",
    "        #Action et état t+1\n",
    "            #a_t\n",
    "            agent1.a_ind = agent1.get_next_action()\n",
    "            agent2.a_ind = agent2.get_next_action()\n",
    "    \n",
    "            #s_t+1\n",
    "            s_t1 = env([agent1.A[agent1.a_ind],agent2.A[agent2.a_ind]])[1]\n",
    "            agent1.s_t1 = s_t1\n",
    "            agent2.s_t1 = s_t1\n",
    "    \n",
    "            s_ind1 = agent1.find_index(agent1.S, agent1.s_t1)\n",
    "            agent1.s_ind1 = s_ind1 \n",
    "            agent2.s_ind1 = s_ind1\n",
    "   \n",
    "            #Alimenter vecteurs temps et reward\n",
    "            temps.append(t)\n",
    "            ret = env(s_t1)\n",
    "            quant, price, cost = ret\n",
    "        \n",
    "            re = ret[0]*ret[1]-ret[0]*ret[2]\n",
    "            reward1.append(re[0])\n",
    "            reward2.append(re[1])\n",
    "            epsilon_value = agent1.epsilon\n",
    "            epsilon.append(epsilon_value)\n",
    "            prices1.append(agent1.p)\n",
    "            prices2.append(agent2.p)\n",
    "    \n",
    "    \n",
    "    \n",
    "            #Les updates pour le prochain tour \n",
    "    \n",
    "            agent1.updateQ(q=quant[0],\n",
    "                       p=price[0],\n",
    "                       c=cost[0],\n",
    "                       t=t)\n",
    "            agent2.updateQ(q=quant[1],\n",
    "                       p=price[1],\n",
    "                       c=cost[1],\n",
    "                       t=t)\n",
    "        \n",
    "        total_reward1.append(reward1)\n",
    "        total_reward2.append(reward2)\n",
    "    aggregated_agent1.append(np.array(total_reward1).mean(axis=0))\n",
    "    aggregated_agent2.append(np.array(total_reward2).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer une figure et des sous-graphiques (3 lignes, 2 colonnes)\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(10, 8))\n",
    "\n",
    "line1, = axes[0, 0].plot(aggregated_agent1[0], alpha=0.5, label = 'Player 1')\n",
    "line2, = axes[0, 0].plot(aggregated_agent2[0], alpha=0.5, label = 'Player 2')\n",
    "axes[0, 0].set_title('$\\\\alpha$ = 0.1')\n",
    "\n",
    "axes[0, 1].plot(aggregated_agent1[1], alpha=0.5, label = 'Player 1')\n",
    "axes[0, 1].plot(aggregated_agent2[1], alpha=0.5, label = 'Player 2')\n",
    "axes[0, 1].set_title('$\\\\alpha$ = 0.2')\n",
    "\n",
    "axes[1, 0].plot(aggregated_agent1[2], alpha=0.5, label = 'Player 1')\n",
    "axes[1, 0].plot(aggregated_agent2[2], alpha=0.5, label = 'Player 2')\n",
    "axes[1, 0].set_title('$\\\\alpha$ = 0.3')\n",
    "\n",
    "axes[1, 1].plot(aggregated_agent1[3], alpha=0.5, label = 'Player 1')\n",
    "axes[1, 1].plot(aggregated_agent2[3], alpha=0.5, label = 'Player 2')\n",
    "axes[1, 1].set_title('$\\\\alpha$ = 0.4')\n",
    "\n",
    "axes[2, 0].plot(aggregated_agent1[4], alpha=0.5, label = 'Player 1')\n",
    "axes[2, 0].plot(aggregated_agent2[4], alpha=0.5, label = 'Player 2')\n",
    "axes[2, 0].set_title('$\\\\alpha$ = 0.5')\n",
    "\n",
    "\n",
    "fig.delaxes(axes[2, 1])\n",
    "\n",
    "lines = [line1, line2]\n",
    "labels = ['Player 1', 'Player 2']\n",
    "fig.legend(lines, labels, loc='upper right', bbox_to_anchor=(0.8,0.2))\n",
    "\n",
    "# Nommer les axes (identique pour tous les sous-graphiques)\n",
    "for ax in axes.flat:\n",
    "    ax.set_xlabel('Number of iterations')\n",
    "    ax.set_ylabel('Reward (3-sample average)')\n",
    "\n",
    "# Titre général de la figure\n",
    "fig.suptitle('Learning curves for $\\\\beta$ = 10e-5, $\\\\delta = 0.95$', fontsize=16)\n",
    "\n",
    "# Ajuster l'espacement entre les sous-graphiques\n",
    "plt.tight_layout()\n",
    "\n",
    "# Afficher la figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RN = 0.23\n",
    "RM = 0.34\n",
    "Rmean1 = np.zeros(5)\n",
    "Rmean2 = np.zeros(5)\n",
    "DRmean1 = np.zeros(5)\n",
    "DRmean2 = np.zeros(5)\n",
    "for i in range (len(aggregated_agent1)): \n",
    "    Rmean1[i] = aggregated_agent1[i][-100:].mean() \n",
    "    Rmean2[i] = aggregated_agent2[i][-100:].mean()\n",
    "    DRmean1[i] = (Rmean1[i] - RN)/(RM - RN) \n",
    "    DRmean2[i] = (Rmean2[i] - RN)/(RM - RN) \n",
    "print(DRmean1)\n",
    "print(DRmean2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRmean12 = [DRmean1,DRmean2]\n",
    "DRmean = np.mean(DRmean12, axis=0)\n",
    "print(DRmean)\n",
    "plt.plot([0.95,0.80,0.65,0.40,0.35],DRmean, linestyle='-', marker='o')\n",
    "plt.xlabel('$\\\\alpha$')\n",
    "plt.ylabel('$\\\\Delta$')\n",
    "plt.title('Average $\\\\Delta$ of players for the last 100 iterations as a function of $\\\\alpha$')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
